{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e5e83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2500)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 2500, 256)    1280256     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       [(None, 2500, 512),  1182720     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     [(None, 2500, 512),  1575936     gru[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    1280256     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     [(None, 2500, 512),  1575936     gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     [(None, None, 512),  1182720     embedding_1[0][0]                \n",
      "                                                                 gru_2[0][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 5001)   2565513     gru_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 10,643,337\n",
      "Trainable params: 10,643,337\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "62/62 [==============================] - 48s 707ms/step - loss: 6.4074 - accuracy: 0.3837\n",
      "Epoch 2/20\n",
      "62/62 [==============================] - 44s 710ms/step - loss: 5.5040 - accuracy: 0.4437\n",
      "Epoch 3/20\n",
      "62/62 [==============================] - 44s 706ms/step - loss: 4.9834 - accuracy: 0.4334\n",
      "Epoch 4/20\n",
      "62/62 [==============================] - 43s 699ms/step - loss: 4.6431 - accuracy: 0.4103\n",
      "Epoch 5/20\n",
      "62/62 [==============================] - 45s 734ms/step - loss: 4.6409 - accuracy: 0.4104\n",
      "Epoch 6/20\n",
      "62/62 [==============================] - 44s 702ms/step - loss: 4.5980 - accuracy: 0.4109\n",
      "Epoch 7/20\n",
      "62/62 [==============================] - 43s 697ms/step - loss: 4.5998 - accuracy: 0.4109\n",
      "Epoch 8/20\n",
      "62/62 [==============================] - 44s 708ms/step - loss: 4.6023 - accuracy: 0.4111\n",
      "Epoch 9/20\n",
      "62/62 [==============================] - 44s 710ms/step - loss: 4.5765 - accuracy: 0.4135\n",
      "Epoch 10/20\n",
      "62/62 [==============================] - 45s 723ms/step - loss: 4.8485 - accuracy: 0.4118\n",
      "Epoch 11/20\n",
      "62/62 [==============================] - 44s 718ms/step - loss: 4.7418 - accuracy: 0.4188\n",
      "Epoch 12/20\n",
      "62/62 [==============================] - 43s 698ms/step - loss: 4.6549 - accuracy: 0.4178\n",
      "Epoch 13/20\n",
      "62/62 [==============================] - 44s 702ms/step - loss: 4.8478 - accuracy: 0.4292\n",
      "Epoch 14/20\n",
      "62/62 [==============================] - 43s 695ms/step - loss: 7.8576 - accuracy: 0.4238\n",
      "Epoch 15/20\n",
      "62/62 [==============================] - 45s 728ms/step - loss: 8.5174 - accuracy: 0.4106\n",
      "Epoch 16/20\n",
      "62/62 [==============================] - 46s 740ms/step - loss: 8.5174 - accuracy: 0.4106\n",
      "Epoch 17/20\n",
      "62/62 [==============================] - 44s 708ms/step - loss: 8.5174 - accuracy: 0.4106\n",
      "Epoch 18/20\n",
      "62/62 [==============================] - 44s 708ms/step - loss: 8.5174 - accuracy: 0.4106\n",
      "Epoch 19/20\n",
      "62/62 [==============================] - 44s 704ms/step - loss: 8.5174 - accuracy: 0.4106\n",
      "Epoch 20/20\n",
      "62/62 [==============================] - 43s 701ms/step - loss: 8.5174 - accuracy: 0.4106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24aa5db1bc8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Embedding, Dense, Concatenate, TimeDistributed\n",
    "import re\n",
    "\n",
    "train_data = pd.read_csv(\"./dataset/TED_Talks_by_ID_plus-transcripts-and-LIWC-and-MFT-plus-views.csv\")\n",
    "def remove_multiple_spaces(s):\n",
    "    return re.sub(r'\\s+', ' ', s)\n",
    "START = '÷'\n",
    "END = '■'\n",
    "punct = '\\r0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{}~'   \n",
    "\n",
    "transtab = str.maketrans(dict.fromkeys(punct, ''))\n",
    "train_data['transcript'] = [START+s.translate(str.maketrans(punct, ' '* len(punct))) + END for s in train_data['transcript'].astype(str)]\n",
    "train_data['transcript'] = train_data['transcript'].apply(remove_multiple_spaces)\n",
    "# print(train_data['transcript'])\n",
    "\n",
    "\n",
    "transtab = str.maketrans(dict.fromkeys(punct, ''))\n",
    "train_data['headline'] = [START+' '  + s.translate(str.maketrans(punct, ' '* len(punct))) + ' '+END for s in train_data['headline'].astype(str)]\n",
    "train_data['headline'] = train_data['headline'].apply(remove_multiple_spaces)\n",
    "# print(train_data['headline'])\n",
    "\n",
    "\n",
    "MAX_LEN_TXT = 2500\n",
    "MAX_LEN = 15\n",
    "latent_dim = 512\n",
    "embedding_dim = 256\n",
    "VOCAB_SIZE = 5000\n",
    "\n",
    "X_tokenizer = Tokenizer(num_words=VOCAB_SIZE+1, oov_token=\"<OOV>\")\n",
    "X_tokenizer.fit_on_texts(train_data['transcript'])\n",
    "adjusted_index_word = {index - 1: word for word, index in X_tokenizer.word_index.items()}\n",
    "X_tokenizer.index_word = adjusted_index_word\n",
    "\n",
    "Y_tokenizer = Tokenizer(num_words=VOCAB_SIZE+1, oov_token=\"<OOV>\")\n",
    "Y_tokenizer.fit_on_texts(train_data['headline'])\n",
    "adjusted_index_word = {index - 1: word for word, index in Y_tokenizer.word_index.items()}\n",
    "Y_tokenizer.index_word = adjusted_index_word\n",
    "\n",
    "X = X_tokenizer.texts_to_sequences(train_data['transcript'])\n",
    "X = pad_sequences(X, maxlen=MAX_LEN_TXT, truncating='post')\n",
    "y = Y_tokenizer.texts_to_sequences(train_data['headline'])\n",
    "y = pad_sequences(y, maxlen=MAX_LEN, truncating='post')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_voc = X_tokenizer.num_words \n",
    "Y_voc = Y_tokenizer.num_words\n",
    "\n",
    "encoder_inputs = Input(shape=(MAX_LEN_TXT,))\n",
    "enc_emb = Embedding(X_voc, embedding_dim, trainable=True)(encoder_inputs)\n",
    "\n",
    "encoder_gru1 = GRU(latent_dim, return_sequences=True, return_state=True, dropout=0.2)\n",
    "encoder_output1, state_h1 = encoder_gru1(enc_emb)\n",
    "\n",
    "encoder_gru2 = GRU(latent_dim, return_sequences=True, return_state=True, dropout=0.2)\n",
    "encoder_output2, state_h2 = encoder_gru2(encoder_output1)\n",
    "\n",
    "encoder_gru3 = GRU(latent_dim, return_sequences=True, return_state=True, dropout=0.4)\n",
    "encoder_outputs, state_h = encoder_gru3(encoder_output2)\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(Y_voc, embedding_dim, trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_gru = GRU(latent_dim, return_sequences=True, return_state=True, dropout=0.4)\n",
    "decoder_outputs, _ = decoder_gru(dec_emb, initial_state=state_h)\n",
    "\n",
    "decoder_dense = TimeDistributed(Dense(Y_voc, activation='relu'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit([X_train, y_train[:, :-1]], y_train[:, 1:], batch_size=32, epochs=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10703e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, state_h)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_inputs = [decoder_state_input_h]\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "decoder_outputs2, state_h2 = decoder_gru(dec_emb2, initial_state=decoder_state_inputs)\n",
    "\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs2] + [state_h2])\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = Y_tokenizer.word_index[START]\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, h = decoder_model.predict([target_seq] + [states_value])\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "#         print(Y_tokenizer.index_word, output_tokens[0, -1, :].shape)\n",
    "        sampled_word = Y_tokenizer.index_word[sampled_token_index]\n",
    "        \n",
    "        decoded_sentence.append(sampled_word)\n",
    "        if sampled_word == END or len(decoded_sentence) > MAX_LEN:\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        states_value = h\n",
    "    return ' '.join(decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c09a864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU Score: 0.35611440760988794\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def bleu_score(model, X_val, y_val):\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    scores = []\n",
    "\n",
    "    for i, input_sequence in enumerate(X_val):\n",
    "        reference = [Y_tokenizer.sequences_to_texts([y_val[i]])[0].split()]\n",
    "        input_sequence = input_sequence.reshape(1, -1)\n",
    "        candidate = decode_sequence(input_sequence).split()\n",
    "        score = sentence_bleu(reference, candidate, smoothing_function=smoothing_function)\n",
    "        scores.append(score)\n",
    "#         print(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "validation_bleu_score = bleu_score(model, X_val, y_val)\n",
    "print(\"Validation BLEU Score:\", validation_bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd922e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
