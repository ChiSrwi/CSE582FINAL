{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d747f9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       ÷ Thank you so much Chris And it s truly a gre...\n",
      "1       ÷ In terms of invention I d like to tell you t...\n",
      "2       ÷ A public Dewey long ago observed is constitu...\n",
      "3       ÷ I want to start off by saying Houston we hav...\n",
      "4       ÷ What I want to talk about is as background i...\n",
      "                              ...                        \n",
      "2470    ÷ Imagine that when you walked in here this ev...\n",
      "2471    ÷ Paying close attention to something Not that...\n",
      "2472    ÷ So this happy pic of me was taken in I was a...\n",
      "2473    ÷ My seven year old grandson sleeps just down ...\n",
      "2474    ÷ Michael Browning engineer innovator inventor...\n",
      "Name: transcript, Length: 2475, dtype: object\n",
      "0                         ÷ Averting the climate crisis ■\n",
      "1                       ÷ Simple designs to save a life ■\n",
      "2                       ÷ How to rebuild a broken state ■\n",
      "3                ÷ The real future of space exploration ■\n",
      "4                            ÷ Great cars are great art ■\n",
      "                              ...                        \n",
      "2470    ÷ Why glass towers are bad for city life and w...\n",
      "2471    ÷ What happens in your brain when you pay atte...\n",
      "2472    ÷ Why you should define your fears instead of ...\n",
      "2473           ÷ truths I learned from life and writing ■\n",
      "2474                           ÷ How I built a jet suit ■\n",
      "Name: headline, Length: 2475, dtype: object\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2500)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 2500, 256)    12800256    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       [(None, 2500, 512),  1182720     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     [(None, 2500, 512),  1575936     gru[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    12800256    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     [(None, 2500, 512),  1575936     gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     [(None, None, 512),  1182720     embedding_1[0][0]                \n",
      "                                                                 gru_2[0][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 50001)  25650513    gru_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 56,768,337\n",
      "Trainable params: 56,768,337\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "62/62 [==============================] - 59s 886ms/step - loss: 5.6706 - accuracy: 0.4041\n",
      "Epoch 2/20\n",
      "62/62 [==============================] - 54s 868ms/step - loss: 4.3563 - accuracy: 0.4106\n",
      "Epoch 3/20\n",
      "62/62 [==============================] - 54s 871ms/step - loss: 4.1277 - accuracy: 0.4692\n",
      "Epoch 4/20\n",
      "62/62 [==============================] - 54s 872ms/step - loss: 4.0532 - accuracy: 0.4818\n",
      "Epoch 5/20\n",
      "62/62 [==============================] - 54s 876ms/step - loss: 4.0144 - accuracy: 0.4818\n",
      "Epoch 6/20\n",
      "62/62 [==============================] - 54s 866ms/step - loss: 3.9901 - accuracy: 0.4826\n",
      "Epoch 7/20\n",
      "62/62 [==============================] - 54s 865ms/step - loss: 3.9653 - accuracy: 0.4886\n",
      "Epoch 8/20\n",
      "62/62 [==============================] - 55s 889ms/step - loss: 3.9318 - accuracy: 0.4959\n",
      "Epoch 9/20\n",
      "62/62 [==============================] - 54s 878ms/step - loss: 3.9066 - accuracy: 0.4981\n",
      "Epoch 10/20\n",
      "62/62 [==============================] - 54s 864ms/step - loss: 3.8918 - accuracy: 0.5008\n",
      "Epoch 11/20\n",
      "62/62 [==============================] - 54s 876ms/step - loss: 3.8665 - accuracy: 0.5025\n",
      "Epoch 12/20\n",
      "62/62 [==============================] - 55s 885ms/step - loss: 3.8384 - accuracy: 0.5032\n",
      "Epoch 13/20\n",
      "62/62 [==============================] - 55s 883ms/step - loss: 3.7992 - accuracy: 0.5031\n",
      "Epoch 14/20\n",
      "62/62 [==============================] - 54s 864ms/step - loss: 3.7664 - accuracy: 0.5077\n",
      "Epoch 15/20\n",
      "62/62 [==============================] - 54s 873ms/step - loss: 3.7389 - accuracy: 0.5111\n",
      "Epoch 16/20\n",
      "62/62 [==============================] - 55s 887ms/step - loss: 3.7115 - accuracy: 0.5141\n",
      "Epoch 17/20\n",
      "62/62 [==============================] - 54s 870ms/step - loss: 3.6858 - accuracy: 0.5148\n",
      "Epoch 18/20\n",
      "62/62 [==============================] - 53s 862ms/step - loss: 3.6605 - accuracy: 0.5185\n",
      "Epoch 19/20\n",
      "62/62 [==============================] - 55s 894ms/step - loss: 3.6712 - accuracy: 0.5026\n",
      "Epoch 20/20\n",
      "62/62 [==============================] - 56s 903ms/step - loss: 3.4513 - accuracy: 0.5210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x226a5817308>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Embedding, Dense, Concatenate, TimeDistributed\n",
    "import re\n",
    "\n",
    "# Load your dataset\n",
    "train_data = pd.read_csv(\"./dataset/TED_Talks_by_ID_plus-transcripts-and-LIWC-and-MFT-plus-views.csv\")\n",
    "\n",
    "def remove_multiple_spaces(s):\n",
    "    return re.sub(r'\\s+', ' ', s)\n",
    "START = '÷'\n",
    "END = '■'\n",
    "\n",
    "punct = '\\r0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{}~'   # `|` is not present here\n",
    "transtab = str.maketrans(dict.fromkeys(punct, ''))\n",
    "# million['transcript'] = '|'.join(million['transcript'].tolist()).translate(transtab).split('|')\n",
    "train_data['transcript'] = [START+s.translate(str.maketrans(punct, ' '* len(punct))) + END for s in train_data['transcript'].astype(str)]\n",
    "train_data['transcript'] = train_data['transcript'].apply(remove_multiple_spaces)\n",
    "print(train_data['transcript'])\n",
    "\n",
    "\n",
    "transtab = str.maketrans(dict.fromkeys(punct, ''))\n",
    "# million['transcript'] = '|'.join(million['transcript'].tolist()).translate(transtab).split('|')\n",
    "train_data['headline'] = [START+' '  + s.translate(str.maketrans(punct, ' '* len(punct))) + ' '+END for s in train_data['headline'].astype(str)]\n",
    "train_data['headline'] = train_data['headline'].apply(remove_multiple_spaces)\n",
    "print(train_data['headline'])\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "MAX_LEN_TXT = 2500\n",
    "MAX_LEN = 15\n",
    "latent_dim = 512\n",
    "embedding_dim = 256\n",
    "VOCAB_SIZE = 50000\n",
    "\n",
    "# Tokenizers\n",
    "X_tokenizer = Tokenizer(num_words=VOCAB_SIZE+1, oov_token=\"<OOV>\")\n",
    "X_tokenizer.fit_on_texts(train_data['transcript'])\n",
    "adjusted_index_word = {index - 1: word for word, index in X_tokenizer.word_index.items()}\n",
    "X_tokenizer.index_word = adjusted_index_word\n",
    "\n",
    "Y_tokenizer = Tokenizer(num_words=VOCAB_SIZE+1, oov_token=\"<OOV>\")\n",
    "Y_tokenizer.fit_on_texts(train_data['headline'])\n",
    "adjusted_index_word = {index - 1: word for word, index in Y_tokenizer.word_index.items()}\n",
    "Y_tokenizer.index_word = adjusted_index_word\n",
    "\n",
    "# Tokenize and pad the input and target sequences\n",
    "X = X_tokenizer.texts_to_sequences(train_data['transcript'])\n",
    "X = pad_sequences(X, maxlen=MAX_LEN_TXT, truncating='post')\n",
    "y = Y_tokenizer.texts_to_sequences(train_data['headline'])\n",
    "y = pad_sequences(y, maxlen=MAX_LEN, truncating='post')\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vocabulary sizes\n",
    "X_voc = X_tokenizer.num_words \n",
    "Y_voc = Y_tokenizer.num_words\n",
    "\n",
    "# Build the GRU model\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(MAX_LEN_TXT,))\n",
    "enc_emb = Embedding(X_voc, embedding_dim, trainable=True)(encoder_inputs)\n",
    "\n",
    "encoder_gru1 = GRU(latent_dim, return_sequences=True, return_state=True, dropout=0.4)\n",
    "encoder_output1, state_h1 = encoder_gru1(enc_emb)\n",
    "\n",
    "encoder_gru2 = GRU(latent_dim, return_sequences=True, return_state=True, dropout=0.4)\n",
    "encoder_output2, state_h2 = encoder_gru2(encoder_output1)\n",
    "\n",
    "encoder_gru3 = GRU(latent_dim, return_sequences=True, return_state=True, dropout=0.4)\n",
    "encoder_outputs, state_h = encoder_gru3(encoder_output2)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(Y_voc, embedding_dim, trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_gru = GRU(latent_dim, return_sequences=True, return_state=True, dropout=0.4)\n",
    "decoder_outputs, _ = decoder_gru(dec_emb, initial_state=state_h)\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(Y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train, y_train[:, :-1]], y_train[:, 1:], batch_size=32, epochs=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a7cb120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Headline: s a to to to to to to a a a the the the the the\n"
     ]
    }
   ],
   "source": [
    "# Inference models\n",
    "\n",
    "# Encoder inference model\n",
    "encoder_model = Model(encoder_inputs, state_h)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_inputs = [decoder_state_input_h]\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "decoder_outputs2, state_h2 = decoder_gru(dec_emb2, initial_state=decoder_state_inputs)\n",
    "\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs2] + [state_h2])\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    # Set the first token of target sequence as the start token.\n",
    "    target_seq[0, 0] = Y_tokenizer.word_index[START]\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, h = decoder_model.predict([target_seq] + [states_value])\n",
    "        \n",
    "#         print(output_tokens[0, -1, :])\n",
    "#         print(Y_tokenizer.index_word)\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = Y_tokenizer.index_word[sampled_token_index]\n",
    "        decoded_sentence.append(sampled_word)\n",
    "        \n",
    "#         print(output_tokens,sampled_token_index, Y_tokenizer.index_word[sampled_token_index])\n",
    "        # Exit condition: either hit max length or find stop token.\n",
    "        if sampled_word == END or len(decoded_sentence) > MAX_LEN:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = h\n",
    "#         print()\n",
    "    return ' '.join(decoded_sentence)\n",
    "\n",
    "# Generate a headline for a given transcript\n",
    "input_sequence = X_val[0:1]\n",
    "generated_headline = decode_sequence(input_sequence)\n",
    "print(\"Generated Headline:\", generated_headline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2adec620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017395797375642234\n",
      "0.014628063653657535\n",
      "0.014628063653657535\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.014628063653657535\n",
      "0\n",
      "0\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.01618861356572822\n",
      "0.01618861356572822\n",
      "0.014628063653657535\n",
      "0.014628063653657535\n",
      "0.012300686288463768\n",
      "0.014628063653657535\n",
      "0.01839381624963888\n",
      "0.030934588294313718\n",
      "0.014628063653657535\n",
      "0.012300686288463768\n",
      "0.014628063653657535\n",
      "0\n",
      "0\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.014628063653657535\n",
      "0.012300686288463768\n",
      "0.01618861356572822\n",
      "0\n",
      "0.014628063653657535\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.014628063653657535\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.01618861356572822\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.014628063653657535\n",
      "0\n",
      "0.014628063653657535\n",
      "0.012300686288463768\n",
      "0.01618861356572822\n",
      "0.012300686288463768\n",
      "0\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.01618861356572822\n",
      "0\n",
      "0\n",
      "0.014628063653657535\n",
      "0.014628063653657535\n",
      "0.01618861356572822\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0\n",
      "0.012300686288463768\n",
      "0\n",
      "0.014628063653657535\n",
      "0\n",
      "0.01618861356572822\n",
      "0.030934588294313718\n",
      "0.014628063653657535\n",
      "0.012300686288463768\n",
      "0.01618861356572822\n",
      "0.014628063653657535\n",
      "0\n",
      "0\n",
      "0\n",
      "0.014628063653657535\n",
      "0.012300686288463768\n",
      "0.014628063653657535\n",
      "0.012300686288463768\n",
      "0\n",
      "0\n",
      "0.012300686288463768\n",
      "0\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0\n",
      "0.014628063653657535\n",
      "0.014628063653657535\n",
      "0\n",
      "0.012300686288463768\n",
      "0\n",
      "0.012300686288463768\n",
      "0\n",
      "0\n",
      "0.014628063653657535\n",
      "0\n",
      "0.01618861356572822\n",
      "0.012300686288463768\n",
      "0.014628063653657535\n",
      "0\n",
      "0.030934588294313718\n",
      "0.014628063653657535\n",
      "0.014628063653657535\n",
      "0.014628063653657535\n",
      "0.014628063653657535\n",
      "0\n",
      "0.02878787818101127\n",
      "0\n",
      "0\n",
      "0.012300686288463768\n",
      "0\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0\n",
      "0\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0\n",
      "0\n",
      "0.012300686288463768\n",
      "0\n",
      "0\n",
      "0\n",
      "0.012300686288463768\n",
      "0\n",
      "0.01618861356572822\n",
      "0.012300686288463768\n",
      "0.014628063653657535\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.012300686288463768\n",
      "0.01618861356572822\n",
      "0.014628063653657535\n",
      "0.014628063653657535\n",
      "0.012300686288463768\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.01618861356572822\n",
      "0.012300686288463768\n",
      "0.014628063653657535\n",
      "0.014628063653657535\n",
      "0.012300686288463768\n",
      "0\n",
      "0.012300686288463768\n",
      "0.030934588294313718\n",
      "0.01618861356572822\n",
      "0.012300686288463768\n",
      "0.01618861356572822\n",
      "0\n",
      "0.014628063653657535\n",
      "0.012300686288463768\n",
      "0.030934588294313718\n",
      "0.014628063653657535\n",
      "0.012300686288463768\n",
      "0\n",
      "0.012300686288463768\n",
      "0.01618861356572822\n",
      "0.014628063653657535\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17120\\2021359368.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Calculate the BLEU score for the validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mvalidation_bleu_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation BLEU Score:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_bleu_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17120\\2021359368.py\u001b[0m in \u001b[0;36mbleu_score\u001b[1;34m(model, X_val, y_val)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mreference\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mY_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequences_to_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0minput_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_sequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mcandidate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_bleu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreference\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmoothing_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msmoothing_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17120\\1448385386.py\u001b[0m in \u001b[0;36mdecode_sequence\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Encode the input as state vectors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mstates_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Generate empty target sequence of length 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mtarget_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\582HW2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1725\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1727\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1728\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\582HW2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\582HW2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\582HW2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3024\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\582HW2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1961\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\582HW2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\582HW2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def bleu_score(model, X_val, y_val):\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    scores = []\n",
    "\n",
    "    for i, input_sequence in enumerate(X_val):\n",
    "        reference = [Y_tokenizer.sequences_to_texts([y_val[i]])[0].split()]\n",
    "        input_sequence = input_sequence.reshape(1, -1)\n",
    "        candidate = decode_sequence(input_sequence).split()\n",
    "        score = sentence_bleu(reference, candidate, smoothing_function=smoothing_function)\n",
    "        scores.append(score)\n",
    "        print(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Calculate the BLEU score for the validation set\n",
    "validation_bleu_score = bleu_score(model, X_val, y_val)\n",
    "print(\"Validation BLEU Score:\", validation_bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e22400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
